\documentclass[11pt,a4paper,oneside]{memoir}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsthm,amsfonts}
\usepackage{url,color}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{calc}
\usepackage{hyperref}

\geometry{hmargin=2cm,vmargin=2.5cm}

\setcounter{secnumdepth}{10}
\setcounter{tocdepth}{10}


%c'est pour avoir N, Z, Q, R, C
\newcommand{\R}{\mathbb{R}}	\newcommand{\Q}{\mathbb{Q}}	\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}	\newcommand{\Z}{\mathbb{Z}}


\theoremstyle{definition}
\newtheorem{definition}{Définition}

\theoremstyle{remark}
\newtheorem*{remarque*}{{Remarque}}

\theoremstyle{plain}
\newtheorem{proposition}{{Proposition}}
\newtheorem*{proposition*}{{Proposition}}
\newtheorem{theorem}{{Théorème}}



%\title{}
%\author{}
%\date{}


\begin{document}
%\maketitle


% ------------- Page de garde ------------------
\thispagestyle{empty}

\begin{minipage}{.47\textwidth}
\centering
\begin{flushleft}
\includegraphics[scale=0.25]{./Images-Rapport/logo_uvsq.jpeg}
\end{flushleft}
\end{minipage}
\begin{minipage}{.47\textwidth}
\centering
\begin{flushright}
\hspace*{1cm} \includegraphics[scale=0.9]{./Images-Rapport/logo_saclay.jpeg}
\end{flushright}
\end{minipage}


\begin{center}
\huge
\textbf{Master 1\\ Calcul Haute Performance et Simulation}
\end{center}


\begin{center}
  \vspace*{\stretch{3}}
  \hrule
  \hrule
  ~\\
  \begin{center}
    \huge
    \textbf{Rapport de stage}\\
    \bigskip
    \textbf{\'Evaluation de performances via des mini-applications}\\
  \end{center}
  ~\\
  \hrule
  \hrule
\end{center}

\vspace*{\stretch{2}}

\begin{center}
	\Large
	\textsc{\textbf{Stagiaire :}}\\
	\textsc{Robin CAZALBOU}\\
	~ \\
	\textsc{\textbf{Encadrant universitaire :}}\\
	\textsc{Pablo OLIVEIRA}\\
	~ \\
	\textsc{\textbf{Tuteur du Centre Borelli :}}\\
	\textsc{Fikri HAFID}\\
\end{center}

\begin{center}
\includegraphics[scale=0.22]{./Images-Rapport/logo_centre_borelli.png}
\end{center}

\vspace*{\stretch{5}}

\begin{center}
\textsc{\textbf{Année universitaire 2020-2021}}
\end{center}

% ----------------------------------------------





\newpage
\tableofcontents



\begin{vplace}[0.5]

\chapter*{Remerciements}
\addcontentsline{toc}{chapter}{Remerciements}

% texte à écrire ici
% fusionner remerciements et introduction




\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

% texte à écrire ici
Ce rapport tient compte des travaux que j'ai réalisé durant mon stage optionnel de fin de master 1 au centre Borelli de l'université Paris Saclay et de l'ENS Paris Saclay. Il fut réalisé sur la période du 17 mai au 28 août 2021, sous la tutelle de M. Fikri HAFID, chercheur au laboratoire du centre Borelli. \bigskip
~\bigskip

% travail en distanciel et en présentiel ?
% ressenti de ce que j'ai pu faire globalement pendant ce stage (premier résumé)

Le cœur de mon travail a consisté en l'étude des mini-applications d'une point de vue Calcul Haute Performance.

% ce que j'ai appris


\end{vplace}







\newpage


\chapter{\'Etat de l'art des mini-applications}

Dans un premier temps, faisons un premier tour d'horizon des mini-applications (que nous appellerons aussi miniapps).

\section{Qu'est ce qu'une miniapp ?}

Une mini-application est définie comme un code de calcul de petite taille (quelques milliers de lignes de code), compilé à partir d'un Makefile simple, cela dans le but de permettre une compréhension plus rapide de l'anatomie interne du code de la part des développeurs \cite{sandia2009}. Ces codes opensource ont pour objectif d'effectuer des simulations simples de problèmes physiques à partir de cœurs d'algorithme de calculs.

Ainsi, la plupart des mini-applications sont obtenues à partir de code de plus grande envergure (par exemple miniXyce, qui simule des circuits électriques, est issue du code Xyce qui comporte plus de 500.000 lignes de codes \cite{sandia2009}). Cependant, contrairement à leur homologue de grande taille, les miniapps n'ont pas pour objectif principal de simuler des problèmes physiques complexes ou inatteignable analytiquement. En effet, la validation des résultats se fait par l'intermédiaire de solutions calculées de façon exacte. L'intérêt sur le résultat à proprement parler est donc infime.

Par ailleurs, les données obtenues après l'exécution du code sont contenues dans des fichiers au format YAML \cite{yaml}, qui possède l'avantage d'être lisible par un humain tout en stockant les informations pour une possible base de données. Des librairies de conception de fichiers au format YAML sont disponibles pour de nombreux langages de programmation.\bigskip

Afin d'être utiles à plusieurs acteurs du monde de la simulation et du Calcul haute performance, ces miniapps se présentent sous la forme simplifiée d'un ou plusieurs \underline{cœurs de calcul numérique}, chacun centré sur une étape clé de l'algorithme global. Par exemple, dans le cas de la mini-application miniFE (pour "mini Finite Elements"), l'étude est centrée sur une méthode des éléments finis, et plus particulièrement sur l'application d'un algorithme de gradient conjugué, qui peut faire l'objet de nombreuses améliorations.

Ces miniapps ont ainsi plusieurs objectifs :
\begin{itemize}
\item permettre aux développeurs HPC et aux numériciens de proposer des méthodes innovantes de communications au sein de l'algorithme, en écartant toutes les données superflues et en permettant une large marge de manœuvre pour effectuer des modifications,
\item permettre d'évaluer les performances d'une architecture de cluster de calcul en considérant la miniapp comme un benchmark simplifié,
\item permettre d'effectuer les modifications du code en conséquences afin de faire concorder matériel et algorithme.
\end{itemize}\bigskip



\section{Présentation de quelques miniapps}

Du fait de la grande diversité des origines des problèmes physiques, ainsi que des approches numériques pour les résoudre, le champs des mini-applications est très vaste. Le Sandia National Laboratories fut un des initiateurs du concept de mini-application et au travers notamment du projet Mantevo \cite{mantevo-project}, de nombreuses miniapps furent conçues. Dans la suite Mantevo 3.0 \cite{heroux2015}, nous pouvons ainsi trouver, entres autres, :
\begin{itemize}
\item \textbf{miniFE} (ou parfois appelée \textbf{HPCCG}) : une mini-application axée sur la résolution d'un système d'équation non-linéaire par la méthode des éléments finis. Le solveur (ou cœur de calcul du code) utilise la méthode du gradient conjugué. Il est écrit en langage C++ et utilise la notion de classe template pour permettre notamment l'étude de l'influence de différents types du C++ permettant le stockage des valeurs (float, double, et leurs variants). Des implémentations hybride MPI+OpenMP, CUDA, OpenCL.
\item \textbf{miniAero} (ajouté à la version 3.0 seulement) : une miniapp de résolution d'équations de Navier-Stokes compressibles, en utilisant des volumes finis au travers d'une méthode de Runge-Kutta d'ordre 4 \cite{miniaero-code}. Le code est écrit en C++.
\item \textbf{TeaLeaf} : écrite en Fortran avec des implémentations OpenMP, MPI et OpenCL, cette miniapp résout l'équation linéaire de la chaleur sur une grille 2D, en utilisant un stencil à 5 points (c'est-à-dire pour lequel chaque cellule mémoire a besoin des données contenues dans les 4 cellules adjacentes). Une version TeaLeaf3D est en développement pour étendre la résolution du problème à un stencil à 7 points. \cite{tealeaf}
\item \textbf{miniGhost} : écrite elle aussi en Fortran et proposant un stencil 5 points, miniGhost étudie notamment plusieurs paradigmes de communications inter-processus en proposant des implémentations sur le modèle BSPMA (Bulk Synchronous Parallel with Message Aggregation), qui consiste à effectuer une phase lourde en communications pendant un intervalle de temps compact, puis à passer à une phase composée uniquement de calculs. D'autres implémentations (SVAF, SVCP) sont proposées et permettent une étude approfondie de ce modèles et de leurs concordances avec l'architecture cible du calculateur \cite{barrett2012}.
\item et bien d'autres (\textbf{miniMD}, \textbf{miniXyce}, \textbf{PathFinder}, \textbf{CoMD}, ...) \cite{heroux2015}
\end{itemize}\bigskip


Afin de pouvoir travailler plus précisément sur le code d'une miniapp en particulier, il m'a fallut faire un choix parmi toutes celles proposées. J'ai écarté celles écrites en Fortran, qui m'auraient demandé un effort supplémentaire dans leur lecture, étant plus familier avec le langage C/C++. Mon choix s'est d'abord penché vers miniAero, dont la segmentation du code permettait une lecture agréable et simplifiée. Cependant l'implémentation proposée par le projet Mantevo \cite{miniaero-code} s'appuyait fortement sur une bibliothèque externe nommée Kokkos, qui exploite un paradigme de programmation du C++ par l'abstraction et l'utilisation massive de templates. Bien que miniAero reste une miniapp du fait que les composantes de Kokkos soient compilées à l'intérieur d'elle, la compréhension de sa structure devenait lourde et aurait impacté les efforts fournis pour comprendre les résultats des mesures effectuées, mais aussi pour éventuellement fournir des améliorations ou modifications du code.\bigskip

J'ai finalement choisi de me concentrer sur la mini-application \textbf{miniXyce}. Cette dernière est un proxy de Xyce, un simulateur de circuit électrique, construit avec un design modulaire et flexible en C++ \cite{xyce}. MiniXyce résout les problèmes linéaires non-symétriques qui apparaissent dans des circuits composés de bobines, de résistances et de condensateurs grâce à une généralisation de la méthode de minimisation des résidus (ou GMRES) et une méthode d'Euler implicite pour l'intégration du temps \cite{minixyce-code}.

Par ailleurs, suite à la sortie de la version 1.0.0 de miniXyce, les développeurs signalent que sa conception a été axée sur la vérification que les résultats retournés étaient corrects, mais que des "analyses approfondies de Xyce ont montré des problèmes de performance sur trois phases de la simulation : le parsing, le chargement/l'évaluation du device, et la solution des équations linéaires" \cite{minixyce-code}.









\chapter{\'Etude de Xyce et miniXyce}

MiniXyce étant issue de Xyce, une étude comparative des deux applications nous permettra d'évaluer les résultats prédits par chacun ainsi que les performances obtenues.

\section{Installation des applications}

Tout d'abord, pour installer et compiler miniXyce à partir des codes sources, les étapes ont été très simples : le dépôt Github \cite{minixyce-code} propose l'ensemble du code source, contenant un Makefile ainsi que les fonctions de calcul, sans appel à une bibliothèque externe (autre que la bibliothèque standard). Un README d'une petite centaine de lignes présente l'ensemble de la miniapp, de son installation et de son utilisation. Cela ne prend que quelques minutes.

\underline{\textbf{Remarque :}} la version disponible sur Github en "release" est la 1.0.0, qui est celle étudiée dans \cite{minixyce-validation}. Plusieurs améliorations de la part de Thornquist ont ensuite été proposées pour obtenir une version 1.1 a priori mieux distribuée selon MPI. De plus, une implémentation utilisant la bibliothèque Kokkos est proposée (en supposant que cette dernière est pré-installée) : nous ne l'étudierons pas car elle dénature le principe de la miniapp, qui est de rester dissociée d'une quelconque bibliothèque externe. Cependant, les quelques essais que j'ai effectué, ainsi que l'analyse du code source, m'ont montré que la version 1.1 ne retourne que les mesures de temps de l'application. Aucun résultat sur le circuit électrique n'est retourné. En l'état, nous continuerons donc notre analyse de miniXyce sur la \textbf{version 1.0.0} .\bigskip


Pour Xyce, les codes sont aussi disponibles sur Github ou sur le site du Sandia National Laboratory \cite{xyce} avec plusieurs notices de plusieurs centaines de pages chacune expliquant en détail la configuration, l'installation et l'utilisation. Du fait de l'ampleur d'un tel code (SLOC $\simeq$ 370 k), la compilation de Xyce seul prend plusieurs dizaines de minutes. Il est à noter aussi que Xyce exploite plusieurs bibliothèques externes, dont BLAS/LAPACK (les bibliothèques d'algèbre linéaire optimisées), FFT, la suite Sparse, ainsi que Trilinos, une bibliothèque conçue aussi par le Sandia Laboratory, et prenant elle aussi plusieurs dizaines de minutes à compiler (après des efforts de configuration).\bigskip

Dans le contexte HPC ou dans l'étude numérique seule, nous pouvons déjà observer que si miniXyce a bien été conçue pour refléter le cœur de calcul de Xyce, son utilisation à la place de Xyce sera nettement plus facilitée.


\section{Résultats obtenus par les applications}

D'après les travaux de Aadithya, Keiter et Thornquist \cite{minixyce-validation}, les résultats obtenus par miniXyce ont pu être validés sur plusieurs circuits différents.

Dans l'ensemble de test de miniXyce 1.0.0, nous pouvons ainsi trouver 5 circuits, ainsi que des scripts de génération d'échelles RC (c'est-à-dire plusieurs condensateurs en parallèles séparés par des résistances) de tailles arbitrairement grande.
Ces 5 circuits représentent des cas simples :
\begin{itemize}
\item circuit 1 : deux couples de résistances branchées en parallèle,
\item circuit 2 : une échelle RC à deux échelons,
\item circuit 3 : un circuit RLC en série,
\item circuit 4 : un circuit RLC simple en parallèle,
\item circuit 5 : un autre circuit RLC plus complexe en parallèle (2 résistances).
\end{itemize}
De plus, différents types de générateurs de courant ou de tension sont utilisés. MiniXyce est réduite aux seuls composants R, L, C, V et I (pour résistance, bobine, condensateur, générateur de tension et générateur de courant respectivement) : nous passons ainsi en revue tous ces éléments.\bigskip

MiniXyce et Xyce fonctionnent tous les deux en ligne de commande (une interface graphique est possible pour Xyce mais n'a pas d'intérêt dans notre étude et ne ferait que ralentir les temps d'exécution). De plus, ils ne nécessitent qu'un seul fichier en entrée, le reste des arguments constitue des options (sur les conditions initiales ou l'instant de début de simulation par exemple). Le fichier, nommé "netlist", contient toutes les informations sur les composants et l'architecture du circuit étudié. Le code source de miniXyce comporte de base les netlists de test issus des observations de \cite{minixyce-validation} et représentant les circuits 1 à 5 présentés précédemment.

Par ailleurs, il est à noter que Xyce et miniXyce proposent une implémentation séquentielle et une autre parallèle. Il sera d'un premier abord intéressant de comparer les performances du cas séquentiel avec l'utilisation de MPI pour un seul processus (et quantifier l'overhead de MPI dans son utilisation en séquentiel).\bigskip


\subsection{Lancement de Xyce}

Le fonctionnement des netlists de Xyce demande beaucoup d'effort à être appréhendé, surtout lorsque l'on n'est pas expert en composants électriques. Je me suis donc basé sur les netlists de test de miniXyce pour reconstruire les netlists des 5 circuits pour Xyce. Que ce soit en version séquentielle ou parallèle, Xyce ressort les mêmes résultats pour un même circuit (comparaison effectuée avec la commande bash "diff").

Précisons au passage que les fonctionnalités de Xyce permettent d'afficher un grand nombre de types de résultats, mais que nous gardons seulement la mesure de la tension aux niveau des nœuds du circuit.

Les résultats obtenus par Xyce et fournis par les concepteurs de miniXyce étant nommés "gold standards", nous avons pu obtenir exactement ces valeurs pour les circuits 1, 2 et 4. Le circuit 5 n'a pas ses gold standards de fournis, et quant au circuit 3, nous avons observé une différence : cela est dû au fait que Xyce choisit elle-même les pas de temps qu'elle va prendre tout au long de l'étude, et seul un pas maximal peut être fixé. Il a donc fallu ruser en imposant à Xyce une liste d'instants de calcul de la tension du circuit. Cependant les valeurs pour ces instants sont obtenus à partir d'une interpolation sur les valeurs calculées selon le choix de Xyce.

Deux questions se posent alors :
\begin{itemize}
\item quel impact cette interpolation a-t-elle sur les performances de Xyce ?

En effet, nous forçons des valeurs spécifiques à être obtenues, et faisons entrer en jeu l'interpolation qui n'est pas disponible sous miniXyce. L'intérêt de l'étude de ce circuit en tant que tel semble ainsi plus limitée.
\item l'écart entre les deux fichiers de valeurs est-il grand ?

\'Etant donné que nous ne savons pas comment le gold standard du circuit 3 a été obtenu, nous devrons nous contenter de valeurs approchées.
\end{itemize}


\`A l'aide d'un petit code que j'ai conçu, nous pouvons ainsi mesurer les écarts des valeurs entre les deux fichiers, ce qui nous donne trois séries de données $S_1$, $S_2$ et $S_3$ (pour les tensions mesurées $V_1$, $V_2$ et $V_3$ en trois nœuds du circuit). Les moyennes et écarts-types calculés sont les suivants :
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Série de données (écarts) & $S_1$ & $S_2$ & $S_3$\\ \hline
Moyenne & $0.000472208$ & $0.000547048$ & $0.000313002$ \\ \hline
\'Ecart-type & $0.000328526$ & $0.000371713$ & $0.000232592$ \\ \hline
\end{tabular}
\end{center}

Nous voyons ainsi que l'interpolation nous donne des résultats très similaires. Il serait intéressant de mesurer l'impact de ce calcul supplémentaire sur le temps d'exécution du code.




\subsection{Lancement de miniXyce}

Précisons qu'avec la version 1.1 de miniXyce, les netlists Xyce sont acceptés (les commandes d'analyses plus poussées sont simplement ignorées par la miniapp). Ce n'est pas le cas pour la version 1.0.0 pour laquelle, par exemple, les commentaires sont symbolisés par un "\%" (là où Xyce les symbolise avec une "*").

Pour vérifier les résultats proposés par miniXyce, nous utilisons les scripts de tests qui sont proposés : ces derniers se basent sur les gold standards (que nous avons vérifié). Un script Perl compare le fichier de sortie Xyce avec le fichier de sortie miniXyce : les 4 circuits passent le test haut-la-main, avec une erreur moyenne sur chaque voltage d'au plus $10^{-10}$.

Les résultats de miniXyce sont plus complets que ceux de Xyce, étant donné qu'ils donnent à la fois le voltage et l'intensité, mais aussi le nombre d'itérations de la méthode GMRES et le nombre de "restarts".





\section{Premiers profils de performance}

Les deux applications présentent de base dans leur code une mesure du temps d'exécution, que ce soit le temps total de la simulation ou bien certaines sous-parties du code qui sont sensées prendre plus de temps.


\subsection{miniXyce}

En analysant le code de miniXyce, nous pouvons remarquer que les mesures de temps forment globalement une partition assez claire du temps total, mise à part sur la boucle principale. Nous trouvons ainsi les mesures de temps suivantes :
\begin{itemize}
\item le parsing des paramètres (conditions initiales, intervalles de temps de la simulation, ...)
\item le parsing du netlist (et donc de la structure du circuit modélisé, des composants en jeu)
\item le calcul du DC Operating Point (ou point de fonctionnement)
\item la mise en place d'une matrice (matrix set up)
\item des I/O pour le résultat de l'application (la tension sur les composants du circuit dans nos exemples)
\item les itérations sur la période transitoire (transient calculation)
\item et enfin la simulation dans son ensemble.
\end{itemize}

Il faut ajouter à cela d'infimes manipulations et allocations de variables entre les mesures (qui, du fait de leur prise de temps très faibles, ne seront pas pertinentes et peuvent être négligées), mais aussi dans le cas de MPI d'un bloc de 6 appels à MPI\_Allreduce, plutôt gourmand en temps.

Enfin, le temps d'exécution de la boucle principale est celui de la période transitoire. Cependant, chaque itération est ponctuée d'une communication inter-processus et d'un I/O de la part du processus 0 : ce temps est compté dans la mesure I/O. Il faut aussi y ajouter une première étape d'I/O avant le début de la période transitoire.

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.55]{Images-Rapport/schéma_miniXyce_1.0.0.png}
\caption{Schéma des différentes sous-parties du code de miniXyce 1.0.0, à partir des mesures de temps qui en sont faites. La partie Allreduce et ajout d'informations au document YAML n'est pas mesurée par la miniapp.}
\label{schéma_miniXyce_1.0.0}
\end{center}
\end{figure}

La figure \ref{schéma_miniXyce_1.0.0} schématise les différentes sous-parties dont la mesure de temps est effectuée de base par le code de miniXyce 1.0.0 .\bigskip

\`A partir de ces mesures, et afin d'avoir des résultat les plus stables possibles, j'ai conçu des scripts et petits codes permettant d'automatiser le lancement de la miniapp 10.000 fois : les résultats des mesures de temps sont récupérés et traités (calcul de moyenne, d'écart-type pour les barres d'erreurs, ainsi que pourcentage du temps par rapport au temps total total de la simulation), puis utilisés pour construire un histogramme résumant ces données (un script Gnuplot permet aussi d'automatiser le processus).\bigskip

% insérer les figures
\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{Images-Rapport/Profil_miniXyce/Profil_miniXyce_sequentiel.png}
\caption{Profil simple de miniXyce, dans le cas séquentiel.}
\label{Profil_miniXyce_sequentiel}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{Images-Rapport/Profil_miniXyce/Profil_miniXyce_parallele1.png}
\includegraphics[scale=0.45]{Images-Rapport/Profil_miniXyce/Profil_miniXyce_parallele2.png}
\includegraphics[scale=0.45]{Images-Rapport/Profil_miniXyce/Profil_miniXyce_parallele3.png}
\includegraphics[scale=0.45]{Images-Rapport/Profil_miniXyce/Profil_miniXyce_parallele4.png}
\caption{Profils simples de miniXyce dans le cas parallèle, pour 1 à 4 processus MPI.}
\label{Profil_miniXyce_parallele}
\end{center}
\end{figure}

Nous retrouvons ainsi dans la figure \ref{Profil_miniXyce_sequentiel} les 7 mesures de temps présentées dans le schéma \ref{schéma_miniXyce_1.0.0}, en échelle logarithmique, et dans le cas séquentiel. Les pourcentages indiquent quelle proportion du temps total est prise par chaque sous-partie de miniXyce. Dans la figure \ref{Profil_miniXyce_parallele}, nous présentons les mêmes mesures dans le cas parallèle, et cela pour des tailles de communicateurs MPI allant de 1 à 4.

Précisons que toutes les mesures ont été faites en simulant les calculs sur le premier circuit présentés plus tôt dans ce chapitre. Ce circuit est très petit (seulement 4 résistances et une source de tension) et comporte peu d'intérêt physique. Cependant, il nous est très utile afin de tester les scripts proposés pour automatiser les mesures de temps et la conception d'un histogramme. Par ailleurs, nous nous proposons pour l'instant de fournir un premier profil du code, qui nous permettra de comprendre dans les grandes lignes les points chauds de miniXyce ainsi que ses faiblesses.

D'autre part, les mesures en multi-processus sont celles rapportées uniquement par le processus 0 : beaucoup de synchronisations sont encore présentes dans le code et les mesures se rapprochent de celles des autres processus. Ce n'est pas optimal pour notre étude, mais comme nous l'avons dit, nous présentons pour l'instant un premier profil simple.

Les mesures ont été réalisées sur ordinateur personnel (à mémoire partagée) à 4 cœurs hyperthreadés Intel i7-4700MQ (Haswell), et avec l'implémentation MPICH 3.4.1 pour MPI. Dans le cas parallèle, l'appel à MPI\_Wtime est utilisé pour les mesures de temps, alors que pour le cas séquentiel, c'est l'appel à gettimeofday qui est réalisé (sachant que sa précision est à la $\mu s$ près).\bigskip

La première chose à relever sur le profil de miniXyce est le fait que la somme des temps ne rejoint pas le temps de simulation total : cela s'explique par certains points du code non évalués, ainsi que par l'inclusion du temps d'I/O dans celui de la période de calcul transitoire.

Nous remarquons néanmoins que la proportion du temps des I/O diminue lorsque le nombre de processus augmente : étant donné que seul le processus 0 s'occupe des I/O, cette baisse est due au fait que le temps global augmente alors que les I/O restent constants. Les appels à MPI\_Allreduce doivent avoir un fort effet sur le temps d'exécution et montrent déjà une mauvaise scalabilité du code.

D'une manière générale, l'optimisation des I/O, la période transitoire et le parsing des paramètres semblent les plus chronophages et méritent le plus d'attention. Une remarque tout de même : notre circuit électrique étant pour l'instant très petit, il ne faut pas négliger la possibilité que la taille de la matrice explose avec le nombre de composants et que sa mise en place deviennent plus conséquente en temps : ce temps est donc à surveiller.




\subsection{Xyce}
























\begin{vplace}[0.5]

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}



\end{vplace}







\begin{vplace}
\chapter*{Annexe}
\addcontentsline{toc}{chapter}{Annexe}

% texte à écrire ici

\end{vplace}




\nocite{*}
\bibliographystyle{plain}
\bibliography{Biblio_Stage_Robin_CAZALBOU}

\end{document}